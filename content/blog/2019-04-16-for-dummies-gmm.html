---
title: "Econometrics for Dummies: GMM"
date: '2019-04-16'
draft: true
categories:
  - econometrics
tags:
  - gmm
output:
  blogdown::html_page:
    toc: yes
    fig_height: 3
    fig_width: 4
---


<div id="TOC">
<ul>
<li><a href="#moments">Moments</a></li>
<li><a href="#method-of-moments">Method of Moments</a><ul>
<li><a href="#a-trivial-example">A trivial example</a></li>
<li><a href="#a-slightly-less-trivial-example">A (slightly) less trivial example</a></li>
<li><a href="#ols-as-an-mom-estimator">OLS as an MOM estimator</a></li>
<li><a href="#single-iv-as-an-mom-estimator">Single-IV as an MOM estimator</a></li>
<li><a href="#links">Links</a></li>
</ul></li>
<li><a href="#generalized-method-of-moments-gmm">Generalized Method of Moments (GMM)</a><ul>
<li><a href="#simple-example-of-gmm">Simple example of GMM</a></li>
<li><a href="#iv-with-multiple-instruments">IV with multiple instruments</a></li>
<li><a href="#links-1">Links</a></li>
</ul></li>
</ul>
</div>

<p>I’m embarassed to admit that a lot “core” econometric concepts are vague to me. I (sometimes) remember a lecture or two on them in graduate school, but having never had need to implement them myself, I would be hard-pressed to describe cogently what they are, let alone code them up. To fill these gaps, this series will review some of these concepts in the simplest terms possible, since that – for me – is the best way to learn.</p>
<p>GMM, or the Generalized Method of Moments, is one such concept. To better understand it, we’ll need to start from some more basic principles that may or may not be familiar. First, we’ll discuss moments, then the (ungeneralized) method of moments, and finally GMM itself.</p>
<div id="moments" class="section level1">
<h1>Moments</h1>
<p>“Moments” are just fancy ways of describing the distibutions of random variables. You’ll see shortly that you’re already familiar with a couple of them. Given a random variable <span class="math inline">\(X\)</span> (and some unrestrictive assumptions about its distribution), the mathematical definition of a “raw moment” is:</p>
<p><span class="math display">\[ E[X^n] \]</span>
The mean is actually the first raw moment, and the only raw moment we really use. We typically define it as <span class="math inline">\(\mu = E[X]\)</span>. Beyond the mean, all of the other moments we use in practice are “central moments”, which are in general given as and are the expectation of the <span class="math inline">\(n\)</span>-exponentiated difference between <span class="math inline">\(X\)</span> and its mean <span class="math inline">\(u\)</span>:</p>
<p><span class="math display">\[ \mu_n = E[(X - \mu)^n] \]</span></p>
<p>The most familiar central moment is the second central moment, which we call the variance. We often write the variance as <span class="math inline">\(\sigma = E[(X - \mu)^2] = E[X^2] - E[X]^2\)</span>. You can get that second equality with a little bit of algebra.</p>
<p>The subsequent central moments are called the “skewness” and “kurtosis”. They can help describe more complex distributions. Note that, for example, the mean and variance and sufficient to describe any normally-distributed random variable.</p>
<p>Besides these population moments, there are sample moments, which look very similar and are defined as follows</p>
<p><span class="math display">\[\begin{align*}
M_k &amp;= \frac{1}{n} \sum_{i=1}^n X_i^k \\ 
M_k^* &amp;= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^k 
\end{align*}\]</span></p>
</div>
<div id="method-of-moments" class="section level1">
<h1>Method of Moments</h1>
<p>The (simple) method of moments is the oldest way to estimate population parameters from observed data. The idea is really simple: suppose you have a random variable from a known distribution with unknown parameters. To estimate those parameters, you just set the population moments equal to the sample moments. It asserts, based on the Law of Large Numbers, that given a set of draws <span class="math inline">\(i = 1, \ldots, N\)</span> from the random variable X, we can estimate the <span class="math inline">\(n\)</span>th population moment with the <span class="math inline">\(n\)</span>th sample moment as follows:</p>
<p><span class="math display">\[ \frac{1}{N} \sum_{i=1}^N X_i^n = E[X^n] \]</span>
If we do this for as many moments as define our data-generating process (here, the distribution), we’ll have <span class="math inline">\(n\)</span> equations with <span class="math inline">\(n\)</span> unknowns, which we can simply solve.</p>
<div id="a-trivial-example" class="section level2">
<h2>A trivial example</h2>
<p>Here’s a trivial example of how this works (<a href="https://newonlinecourses.science.psu.edu/stat414/node/193/">source</a>). Suppose we observe a set of random variables <span class="math inline">\(X_1, \ldots, X_n\)</span> drawn from a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. We want to derive the estimators for these parameters using the method of moments. First, we can compute the population moments. We’ll use the first moment about the origin and the second moment about the mean. These are</p>
<p><span class="math display">\[ E[X] = \mu \]</span>
<span class="math display">\[ E[X^2] - E[X]^2 = \sigma^2 \]</span></p>
<p>Next, we compute the sample moments. These are going to look very similar, except now these represent real observations from our sample.</p>
<p><span class="math display">\[ M_1 = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X} \]</span>
<span class="math display">\[ M_2^* = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \]</span></p>
<p>The “trick” to the method of moments is that we can just set the population moments equal to the sample moments and solve for the parameters <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span>, which now have hats because they’re estimators. So we have:</p>
<p><span class="math display">\[\hat{\mu} = \bar{X}\]</span></p>
<p>and</p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2\]</span></p>
<p>Now we solve for <span class="math inline">\(\hat{\mu}\)</span> and <span class="math inline">\(\hat{\sigma^2}\)</span>. But wait – these equations have already been solved! So we’re done.</p>
</div>
<div id="a-slightly-less-trivial-example" class="section level2">
<h2>A (slightly) less trivial example</h2>
<p>That feels a bit too on the nose to be informative, so let’s try this again but instead we’ll use the raw moments for both the first and second moments. Skipping to the second moment (since we already did the first raw moment above), we get the population moment:</p>
<p><span class="math display">\[ E[X^2] = E[X^2] - E[X]^2 + E[X]^2 = Var(X) + E[X]^2 = \sigma^2 + \mu^2 \]</span></p>
<p>Note that we used the definition of population variance, AKA the central moment, to derive this formula for the raw variance. Now we can compute the sample equivalent:</p>
<p><span class="math display">\[ M_2 = \frac{1}{n} \sum_{i = 1}^{n} X_i^2 \]</span>
Setting this equal to our expression above gives</p>
<p><span class="math display">\[\begin{align*}
\sigma^2 + \mu^2 &amp;=  \frac{1}{n} \sum_{i = 1}^{n} X_i^2 \\
\sigma^2 &amp;=  \frac{1}{n} \sum_{i = 1}^{n} X_i^2 - \mu^2 \\
\hat{\sigma}^2 &amp;= \frac{1}{n} \sum_{i = 1}^{n} (X_i - \bar{X})^2
\end{align*}\]</span></p>
<p>Note that getting from the second to the third lines requires a few algebraic steps that I skip.</p>
</div>
<div id="ols-as-an-mom-estimator" class="section level2">
<h2>OLS as an MOM estimator</h2>
<p>Start with a data generating process that takes the following form:</p>
<p><span class="math display">\[ y = \alpha + \beta x + u \]</span></p>
<p>Note that we want to estimate both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, so we need <em>two</em> moment conditions. We’ll start with the (benign) assumption that <span class="math inline">\(E[u] = 0\)</span>. This assumption is benign because we included the intercept, <span class="math inline">\(\alpha\)</span>.</p>
<p>The other assumption we’ll need is that <span class="math inline">\(\text{Cov}(x, u) = 0\)</span>. This isn’t so benign. This is an <em>unconfoundedness</em> assumption, which basically asserts that this model doesn’t suffer from any endogeneity problems. Since this is a blog post and not an econometrics class, that’s as far as we’ll go on that.</p>
<p><span class="math display">\[\begin{align*}
E[u] &amp;= 0 \\
&amp;= E[y - \alpha - \beta x] \\
&amp;= E[y] - E[\alpha] - E[\beta x] \\
\alpha &amp;= E[y] - \beta E[x]
\end{align*}\]</span>
This is our estimator for <span class="math inline">\(\alpha\)</span>, but we need to come up with an estimator for <span class="math inline">\(\beta\)</span> as well.</p>
<p>To do this, we start with our second moment condition and the definition of covariance:
<span class="math display">\[\begin{align*}
\text{Cov}(x,u) &amp;= E[(x - E[x])(u - E[u])] \\
&amp;= E[xu - uE[x]] \\
&amp;= E[xu] - E[x]E[u] \\
&amp;= E[xu] \\
&amp;= E[x(y - \alpha - \beta x)] \\
&amp;= E[xy] - \alpha E[x] - \beta E[x^2] \\
&amp;= E[xy] - (E[y] - \beta E[x]) E[x] - \beta E[x^2] \\
&amp;= E[xy] - E[x]E[y] - \beta (E[x]^2 - E[x^2]) \\
\beta &amp;= \frac{E[xy] - E[x]E[y]}{E[x]^2 - E[x^2]} \\
\end{align*}\]</span>
The above also uses that <span class="math inline">\(E[u] = 0\)</span> and that <span class="math inline">\(E[x]\)</span> is a constant. This should look familiar. This is the same result you can obtain from the first order conditions for OLS, but no calculus was required. From here, we simply put hats on <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, replace the population expectations with sample expectations, and call it a day.</p>
<p>Reference: <a href="http://www.nathanielhiggins.com/uploads/3/6/4/2/3642182/deriving-ols.pdf" class="uri">http://www.nathanielhiggins.com/uploads/3/6/4/2/3642182/deriving-ols.pdf</a></p>
</div>
<div id="single-iv-as-an-mom-estimator" class="section level2">
<h2>Single-IV as an MOM estimator</h2>
</div>
<div id="links" class="section level2">
<h2>Links</h2>
<ul>
<li>Method of moments <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">wiki</a></li>
<li>RandomServices <a href="http://www.randomservices.org/random/point/Moments.html">method of moments</a></li>
<li>Berkeley Statistics <a href="https://www.stat.berkeley.edu/~vigre/activities/bootstrap/2006/wickham_stati.pdf">notes</a></li>
</ul>
</div>
</div>
<div id="generalized-method-of-moments-gmm" class="section level1">
<h1>Generalized Method of Moments (GMM)</h1>
<p>One key to the method of moments estimator is that we have the same number of moment conditions as parameters to estimate. But what if we have <em>more</em> moments than parameters, as is the case in some regression settings (e.g., when one has multiple instruments). GMM helps us combine all of these moments optimally.</p>
<div id="simple-example-of-gmm" class="section level2">
<h2>Simple example of GMM</h2>
</div>
<div id="iv-with-multiple-instruments" class="section level2">
<h2>IV with multiple instruments</h2>
</div>
<div id="links-1" class="section level2">
<h2>Links</h2>
<ul>
<li>Penn State <a href="https://newonlinecourses.science.psu.edu/stat414/node/193/">STAT 414/415</a></li>
<li>StackExchange: <a href="https://stats.stackexchange.com/questions/287/what-is-the-difference-relationship-between-method-of-moments-and-gmm">Difference between Method of Moments and GMM</a></li>
<li><a href="https://www.youtube.com/watch?v=pIIEmUEnjhY" class="uri">https://www.youtube.com/watch?v=pIIEmUEnjhY</a></li>
<li><a href="https://www.stata.com/meeting/germany10/germany10_drukker.pdf" class="uri">https://www.stata.com/meeting/germany10/germany10_drukker.pdf</a></li>
</ul>
<p>Next up… Maximum Likelihood Estimation!
- Peter Zsohar’s GMM <a href="http://www.ksh.hu/statszemle_archive/2012/2012_K16/2012_K16_150.pdf">notes</a></p>
</div>
</div>
